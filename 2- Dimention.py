# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UDyCobyu23em9TbpWkw2kgiTdTEinyIW
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
from numpy.linalg import pinv

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Define the range for the number of components
range_n_components = range(2, X_wine_train.shape[1] + 1)

# Compute reconstruction error for different number of components
reconstruction_errors = []

for n_components in range_n_components:
    rp = GaussianRandomProjection(n_components=n_components, random_state=42)
    X_wine_train_rp = rp.fit_transform(X_wine_train)
    # Reconstruct the original data
    projection_matrix = rp.components_.T
    X_wine_train_reconstructed = X_wine_train_rp @ projection_matrix.T
    reconstruction_error = np.mean((X_wine_train - X_wine_train_reconstructed) ** 2)
    reconstruction_errors.append(reconstruction_error)

# Plot the reconstruction error results
plt.figure(figsize=(10, 5))
plt.plot(range_n_components, reconstruction_errors, marker='o')
plt.title('Reconstruction Error for Randomized Projections - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Reconstruction Error')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 8

# Apply Randomized Projections for 2D visualization
rp_wine_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_wine_train_rp_2d = rp_wine_2d.fit_transform(X_wine_train)

# Apply Randomized Projections for 3D visualization
rp_wine_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_wine_train_rp_3d = rp_wine_3d.fit_transform(X_wine_train)

# 2D Plot for Wine Dataset
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_rp_2d[:, 0], X_wine_train_rp_2d[:, 1], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('Randomized Projections (2D) - Wine Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Quality')
plt.show()

# 3D Plot for Wine Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_rp_3d[:, 0], X_wine_train_rp_3d[:, 1], X_wine_train_rp_3d[:, 2], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('Randomized Projections (3D) - Wine Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Quality')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Apply PCA
pca = PCA()
X_wine_train_pca = pca.fit_transform(X_wine_train)

# Determine the optimal number of components based on explained variance
explained_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot the explained variance
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')
plt.title('Explained Variance for PCA - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.grid(True)
plt.show()

# Determine the optimal number of components (e.g., 95% variance)
optimal_n_components = np.argmax(explained_variance >= 0.95) + 1
print(f'Optimal number of components for 95% variance: {optimal_n_components}')

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 9

# Apply PCA with optimal number of components
pca_optimal = PCA(n_components=optimal_n_components)
X_wine_train_pca_optimal = pca_optimal.fit_transform(X_wine_train)

# 2D Plot for Wine Dataset (first two principal components)
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_pca_optimal[:, 0], X_wine_train_pca_optimal[:, 1], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('PCA (2D) - Wine Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Quality')
plt.show()

# Apply PCA for 3D visualization
pca_3d = PCA(n_components=3)
X_wine_train_pca_3d = pca_3d.fit_transform(X_wine_train)

# 3D Plot for Wine Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_pca_3d[:, 0], X_wine_train_pca_3d[:, 1], X_wine_train_pca_3d[:, 2], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('PCA (3D) - Wine Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Quality')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from scipy.stats import kurtosis
import matplotlib.pyplot as plt

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Define the range for the number of components
range_n_components = range(2, X_wine_train.shape[1] + 1)

# Compute kurtosis for different number of components
kurtosis_values = []

for n_components in range_n_components:
    ica = FastICA(n_components=n_components, random_state=42)
    X_wine_train_ica = ica.fit_transform(X_wine_train)
    kurt = np.mean(kurtosis(X_wine_train_ica, axis=0))
    kurtosis_values.append(kurt)

# Plot the kurtosis results
plt.figure(figsize=(10, 5))
plt.plot(range_n_components, kurtosis_values, marker='o')
plt.title('Kurtosis for ICA - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Kurtosis')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 10

# Apply ICA with optimal number of components
ica_optimal = FastICA(n_components=optimal_n_components, random_state=42)
X_wine_train_ica_optimal = ica_optimal.fit_transform(X_wine_train)

# 2D Plot for Wine Dataset (first two independent components)
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_ica_optimal[:, 0], X_wine_train_ica_optimal[:, 1], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('ICA (2D) - Wine Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Quality')
plt.show()

# Apply ICA for 3D visualization
ica_3d = FastICA(n_components=3, random_state=42)
X_wine_train_ica_3d = ica_3d.fit_transform(X_wine_train)

# 3D Plot for Wine Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_ica_3d[:, 0], X_wine_train_ica_3d[:, 1], X_wine_train_ica_3d[:, 2], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
plt.title('ICA (3D) - Wine Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Quality')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
from numpy.linalg import pinv

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Define the range for the number of components
range_n_components = range(2, X_breast_train.shape[1] + 1)

# Compute reconstruction error for different number of components
reconstruction_errors = []

for n_components in range_n_components:
    rp = GaussianRandomProjection(n_components=n_components, random_state=42)
    X_breast_train_rp = rp.fit_transform(X_breast_train)
    # Reconstruct the original data
    projection_matrix = rp.components_.T
    X_breast_train_reconstructed = X_breast_train_rp @ projection_matrix.T
    reconstruction_error = np.mean((X_breast_train - X_breast_train_reconstructed) ** 2)
    reconstruction_errors.append(reconstruction_error)

# Plot the reconstruction error results
plt.figure(figsize=(10, 5))
plt.plot(range_n_components, reconstruction_errors, marker='o')
plt.title('Reconstruction Error for Randomized Projections - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Reconstruction Error')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 7

# Apply Randomized Projections for 2D visualization
rp_breast_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_breast_train_rp_2d = rp_breast_2d.fit_transform(X_breast_train)

# Apply Randomized Projections for 3D visualization
rp_breast_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_breast_train_rp_3d = rp_breast_3d.fit_transform(X_breast_train)

# 2D Plot for Breast Cancer Dataset
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_rp_2d[:, 0], X_breast_train_rp_2d[:, 1], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('Randomized Projections (2D) - Breast Cancer Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Class')
plt.show()

# 3D Plot for Breast Cancer Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_rp_3d[:, 0], X_breast_train_rp_3d[:, 1], X_breast_train_rp_3d[:, 2], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('Randomized Projections (3D) - Breast Cancer Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Class')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Apply PCA
pca = PCA()
X_breast_train_pca = pca.fit_transform(X_breast_train)

# Determine the optimal number of components based on explained variance
explained_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot the explained variance
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')
plt.title('Explained Variance for PCA - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.grid(True)
plt.show()

# Determine the optimal number of components (e.g., 95% variance)
optimal_n_components = np.argmax(explained_variance >= 0.95) + 1
print(f'Optimal number of components for 95% variance: {optimal_n_components}')

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 8

# Apply PCA with optimal number of components
pca_optimal = PCA(n_components=optimal_n_components)
X_breast_train_pca_optimal = pca_optimal.fit_transform(X_breast_train)

# 2D Plot for Breast Cancer Dataset (first two principal components)
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_pca_optimal[:, 0], X_breast_train_pca_optimal[:, 1], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('PCA (2D) - Breast Cancer Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Class')
plt.show()

# Apply PCA for 3D visualization
pca_3d = PCA(n_components=3)
X_breast_train_pca_3d = pca_3d.fit_transform(X_breast_train)

# 3D Plot for Breast Cancer Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_pca_3d[:, 0], X_breast_train_pca_3d[:, 1], X_breast_train_pca_3d[:, 2], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('PCA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Class')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from scipy.stats import kurtosis
import matplotlib.pyplot as plt

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Define the range for the number of components
range_n_components = range(2, X_breast_train.shape[1] + 1)

# Compute kurtosis for different number of components
kurtosis_values = []

for n_components in range_n_components:
    ica = FastICA(n_components=n_components, random_state=42)
    X_breast_train_ica = ica.fit_transform(X_breast_train)
    kurt = np.mean(kurtosis(X_breast_train_ica, axis=0))
    kurtosis_values.append(kurt)

# Plot the kurtosis results
plt.figure(figsize=(10, 5))
plt.plot(range_n_components, kurtosis_values, marker='o')
plt.title('Kurtosis for ICA - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Kurtosis')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components based on the plot
optimal_n_components = 8

# Apply ICA with optimal number of components
ica_optimal = FastICA(n_components=optimal_n_components, random_state=42)
X_breast_train_ica_optimal = ica_optimal.fit_transform(X_breast_train)

# 2D Plot for Breast Cancer Dataset (first two independent components)
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_ica_optimal[:, 0], X_breast_train_ica_optimal[:, 1], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('ICA (2D) - Breast Cancer Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Class')
plt.show()

# Apply ICA for 3D visualization
ica_3d = FastICA(n_components=3, random_state=42)
X_breast_train_ica_3d = ica_3d.fit_transform(X_breast_train)

# 3D Plot for Breast Cancer Dataset
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_ica_3d[:, 0], X_breast_train_ica_3d[:, 1], X_breast_train_ica_3d[:, 2], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
plt.title('ICA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Class')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
from numpy.linalg import pinv
import matplotlib.pyplot as plt

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')
wine = pd.concat([wine_red, wine_white], ignore_index=True)
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']
scaler_wine = StandardScaler()
X_wine_scaled = scaler_wine.fit_transform(X_wine)
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Load the breast cancer dataset
column_names = ["Class", "age", "menopause", "tumor-size", "inv-nodes", "node-caps", "deg-malig", "breast", "breast-quad", "irradiat"]
breast_cancer = pd.read_csv('/content/breast/breast-cancer.data', header=None, names=column_names)
for column in breast_cancer.columns:
    breast_cancer[column] = breast_cancer[column].astype('category').cat.codes
X_breast = breast_cancer.drop(columns=['Class'])
y_breast = breast_cancer['Class']
scaler_breast = StandardScaler()
X_breast_scaled = scaler_breast.fit_transform(X_breast)
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Determine reconstruction error for different number of components
n_components = np.arange(2, 10)
reconstruction_errors_wine = []
reconstruction_errors_breast = []

for n in n_components:
    rp_wine = GaussianRandomProjection(n_components=n, random_state=42)
    X_wine_train_rp = rp_wine.fit_transform(X_wine_train)
    X_wine_train_reconstructed = X_wine_train_rp @ pinv(rp_wine.components_.T)
    reconstruction_error_wine = np.mean((X_wine_train - X_wine_train_reconstructed) ** 2)
    reconstruction_errors_wine.append(reconstruction_error_wine)

    rp_breast = GaussianRandomProjection(n_components=n, random_state=42)
    X_breast_train_rp = rp_breast.fit_transform(X_breast_train)
    X_breast_train_reconstructed = X_breast_train_rp @ pinv(rp_breast.components_.T)
    reconstruction_error_breast = np.mean((X_breast_train - X_breast_train_reconstructed) ** 2)
    reconstruction_errors_breast.append(reconstruction_error_breast)

# Create a combined plot
plt.figure(figsize=(14, 6))

# Plot reconstruction error for Wine dataset
plt.subplot(1, 2, 1)
plt.plot(n_components, reconstruction_errors_wine, marker='o')
plt.title('Reconstruction Error for Randomized Projections - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Reconstruction Error')
plt.grid(True)

# Plot reconstruction error for Breast Cancer dataset
plt.subplot(1, 2, 2)
plt.plot(n_components, reconstruction_errors_breast, marker='o')
plt.title('Reconstruction Error for Randomized Projections - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Reconstruction Error')
plt.grid(True)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')
wine = pd.concat([wine_red, wine_white], ignore_index=True)
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']
scaler_wine = StandardScaler()
X_wine_scaled = scaler_wine.fit_transform(X_wine)
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Load the breast cancer dataset
column_names = ["Class", "age", "menopause", "tumor-size", "inv-nodes", "node-caps", "deg-malig", "breast", "breast-quad", "irradiat"]
breast_cancer = pd.read_csv('/content/breast/breast-cancer.data', header=None, names=column_names)
for column in breast_cancer.columns:
    breast_cancer[column] = breast_cancer[column].astype('category').cat.codes
X_breast = breast_cancer.drop(columns=['Class'])
y_breast = breast_cancer['Class']
scaler_breast = StandardScaler()
X_breast_scaled = scaler_breast.fit_transform(X_breast)
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Determine explained variance for different number of components
n_components_wine = np.arange(1, X_wine_train.shape[1] + 1)
n_components_breast = np.arange(1, X_breast_train.shape[1] + 1)
explained_variance_wine = []
explained_variance_breast = []

for n in n_components_wine:
    pca_wine = PCA(n_components=n)
    pca_wine.fit(X_wine_train)
    explained_variance_wine.append(np.sum(pca_wine.explained_variance_ratio_))

for n in n_components_breast:
    pca_breast = PCA(n_components=n)
    pca_breast.fit(X_breast_train)
    explained_variance_breast.append(np.sum(pca_breast.explained_variance_ratio_))

# Create a combined plot
plt.figure(figsize=(14, 6))

# Plot explained variance for Wine dataset
plt.subplot(1, 2, 1)
plt.plot(n_components_wine, explained_variance_wine, marker='o')
plt.title('Explained Variance for PCA - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Explained Variance')
plt.grid(True)

# Plot explained variance for Breast Cancer dataset
plt.subplot(1, 2, 2)
plt.plot(n_components_breast, explained_variance_breast, marker='o')
plt.title('Explained Variance for PCA - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Explained Variance')
plt.grid(True)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
import matplotlib.pyplot as plt
from scipy.stats import kurtosis

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')
wine = pd.concat([wine_red, wine_white], ignore_index=True)
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']
scaler_wine = StandardScaler()
X_wine_scaled = scaler_wine.fit_transform(X_wine)
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Load the breast cancer dataset
column_names = ["Class", "age", "menopause", "tumor-size", "inv-nodes", "node-caps", "deg-malig", "breast", "breast-quad", "irradiat"]
breast_cancer = pd.read_csv('/content/breast/breast-cancer.data', header=None, names=column_names)
for column in breast_cancer.columns:
    breast_cancer[column] = breast_cancer[column].astype('category').cat.codes
X_breast = breast_cancer.drop(columns=['Class'])
y_breast = breast_cancer['Class']
scaler_breast = StandardScaler()
X_breast_scaled = scaler_breast.fit_transform(X_breast)
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Determine kurtosis for different number of components
n_components_wine = np.arange(2, X_wine_train.shape[1] + 1)
n_components_breast = np.arange(2, X_breast_train.shape[1] + 1)
kurtosis_wine = []
kurtosis_breast = []

for n in n_components_wine:
    ica_wine = FastICA(n_components=n, random_state=42)
    X_wine_train_ica = ica_wine.fit_transform(X_wine_train)
    k = np.mean(np.abs(kurtosis(X_wine_train_ica)))
    kurtosis_wine.append(k)

for n in n_components_breast:
    ica_breast = FastICA(n_components=n, random_state=42)
    X_breast_train_ica = ica_breast.fit_transform(X_breast_train)
    k = np.mean(np.abs(kurtosis(X_breast_train_ica)))
    kurtosis_breast.append(k)

# Create a combined plot
plt.figure(figsize=(14, 6))

# Plot kurtosis for Wine dataset
plt.subplot(1, 2, 1)
plt.plot(n_components_wine, kurtosis_wine, marker='o')
plt.title('Kurtosis for ICA - Wine Dataset')
plt.xlabel('Number of components')
plt.ylabel('Kurtosis')
plt.grid(True)

# Plot kurtosis for Breast Cancer dataset
plt.subplot(1, 2, 2)
plt.plot(n_components_breast, kurtosis_breast, marker='o')
plt.title('Kurtosis for ICA - Breast Cancer Dataset')
plt.xlabel('Number of components')
plt.ylabel('Kurtosis')
plt.grid(True)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')
wine = pd.concat([wine_red, wine_white], ignore_index=True)
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']
scaler_wine = StandardScaler()
X_wine_scaled = scaler_wine.fit_transform(X_wine)
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Load the breast cancer dataset
column_names = ["Class", "age", "menopause", "tumor-size", "inv-nodes", "node-caps", "deg-malig", "breast", "breast-quad", "irradiat"]
breast_cancer = pd.read_csv('/content/breast/breast-cancer.data', header=None, names=column_names)
for column in breast_cancer.columns:
    breast_cancer[column] = breast_cancer[column].astype('category').cat.codes
X_breast = breast_cancer.drop(columns=['Class'])
y_breast = breast_cancer['Class']
scaler_breast = StandardScaler()
X_breast_scaled = scaler_breast.fit_transform(X_breast)
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Apply Randomized Projections
rp_wine = GaussianRandomProjection(n_components=3, random_state=42)
X_wine_train_rp = rp_wine.fit_transform(X_wine_train)

rp_breast = GaussianRandomProjection(n_components=3, random_state=42)
X_breast_train_rp = rp_breast.fit_transform(X_breast_train)

# Create a combined 3D plot
fig = plt.figure(figsize=(14, 6))

# 3D plot for Wine dataset
ax1 = fig.add_subplot(121, projection='3d')
scatter1 = ax1.scatter(X_wine_train_rp[:, 0], X_wine_train_rp[:, 1], X_wine_train_rp[:, 2], c=y_wine_train, cmap='viridis', s=50, alpha=0.6)
legend1 = ax1.legend(*scatter1.legend_elements(), title="Quality")
ax1.add_artist(legend1)
ax1.set_title('Randomized Projections (3D) - Wine Dataset')
ax1.set_xlabel('Component 1')
ax1.set_ylabel('Component 2')
ax1.set_zlabel('Component 3')
plt.colorbar(scatter1, ax=ax1, pad=0.1)

# 3D plot for Breast Cancer dataset
ax2 = fig.add_subplot(122, projection='3d')
scatter2 = ax2.scatter(X_breast_train_rp[:, 0], X_breast_train_rp[:, 1], X_breast_train_rp[:, 2], c=y_breast_train, cmap='viridis', s=50, alpha=0.6)
legend2 = ax2.legend(*scatter2.legend_elements(), title="Class")
ax2.add_artist(legend2)
ax2.set_title('Randomized Projections (3D) - Breast Cancer Dataset')
ax2.set_xlabel('Component 1')
ax2.set_ylabel('Component 2')
ax2.set_zlabel('Component 3')
plt.colorbar(scatter2, ax=ax2, pad=0.1)

plt.tight_layout()
plt.show()