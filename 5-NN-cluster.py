# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lfTf81sZS5qsvMKILj8wCuKhPTHic-DB
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X = wine.drop(columns=['quality'])
y = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.mixture import GaussianMixture

# Fit the Gaussian Mixture Model (EM algorithm)
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X_train)

# Add the cluster labels as new features
X_train_em = pd.DataFrame(X_train)
X_test_em = pd.DataFrame(X_test)
X_train_em['EM_Cluster'] = gmm.predict(X_train)
X_test_em['EM_Cluster'] = gmm.predict(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score

# Define the Neural Network model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_em.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

# Train the model and capture computation time
start_time = time.time()
history = model.fit(X_train_em, y_train, epochs=100, batch_size=32, validation_data=(X_test_em, y_test), verbose=1)
end_time = time.time()
print(f"Computation time for training the Neural Network with EM Clustering: {end_time - start_time:.4f} seconds")

# Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss - Expectation Maximization in NN')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.grid(True)
plt.show()

# Convert regression output to classification (for the sake of plotting accuracy)
def convert_to_classification(y, threshold=5.5):
    return (y >= threshold).astype(int)

train_sizes = np.linspace(0.1, 0.9, 9)  # Avoiding 1.0 to prevent the error
train_accuracies = []
test_accuracies = []

for train_size in train_sizes:
    X_train_part, _, y_train_part, _ = train_test_split(X_train_em, y_train, train_size=train_size, random_state=42)

    model.fit(X_train_part, y_train_part, epochs=100, batch_size=32, verbose=0)
    y_train_pred = convert_to_classification(model.predict(X_train_part))
    y_test_pred = convert_to_classification(model.predict(X_test_em))

    train_accuracies.append(accuracy_score(convert_to_classification(y_train_part), y_train_pred))
    test_accuracies.append(accuracy_score(convert_to_classification(y_test), y_test_pred))

# Plot training size vs. accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_sizes, train_accuracies, marker='o', label='Train')
plt.plot(train_sizes, test_accuracies, marker='o', label='Test')
plt.title('Training Size vs. Accuracy - Expectation Maximization in NN')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X = wine.drop(columns=['quality'])
y = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
from sklearn.cluster import KMeans

# Fit the K-Means model
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_train)

# Add the cluster labels as new features
X_train_kmeans = pd.DataFrame(X_train)
X_test_kmeans = pd.DataFrame(X_test)
X_train_kmeans['KMeans_Cluster'] = kmeans.predict(X_train)
X_test_kmeans['KMeans_Cluster'] = kmeans.predict(X_test)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score

# Define the Neural Network model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_kmeans.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

# Train the model and capture computation time
start_time = time.time()
history = model.fit(X_train_kmeans, y_train, epochs=100, batch_size=32, validation_data=(X_test_kmeans, y_test), verbose=1)
end_time = time.time()
print(f"Computation time for training the Neural Network with K-Means Clustering: {end_time - start_time:.4f} seconds")

# Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss - K-means in NN')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.grid(True)
plt.show()

# Convert regression output to classification (for the sake of plotting accuracy)
def convert_to_classification(y, threshold=5.5):
    return (y >= threshold).astype(int)

train_sizes = np.linspace(0.1, 0.9, 9)  # Avoiding 1.0 to prevent the error
train_accuracies = []
test_accuracies = []

for train_size in train_sizes:
    X_train_part, _, y_train_part, _ = train_test_split(X_train_kmeans, y_train, train_size=train_size, random_state=42)

    model.fit(X_train_part, y_train_part, epochs=100, batch_size=32, verbose=0)
    y_train_pred = convert_to_classification(model.predict(X_train_part))
    y_test_pred = convert_to_classification(model.predict(X_test_kmeans))

    train_accuracies.append(accuracy_score(convert_to_classification(y_train_part), y_train_pred))
    test_accuracies.append(accuracy_score(convert_to_classification(y_test), y_test_pred))

# Plot training size vs. accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_sizes, train_accuracies, marker='o', label='Train')
plt.plot(train_sizes, test_accuracies, marker='o', label='Test')
plt.title('Training Size vs. Accuracy - K-means in NN')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()