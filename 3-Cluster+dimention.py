# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LquLrU-J2Doq_35bSIEgpNMukdHdjSpG
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for Randomized Projections
optimal_n_components_rp = 8

# Apply Randomized Projections for 2D visualization
rp_wine_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_wine_train_rp_2d = rp_wine_2d.fit_transform(X_wine_train)

# Apply Randomized Projections for 3D visualization
rp_wine_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_wine_train_rp_3d = rp_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_2d = gmm_2d.fit_predict(X_wine_train_rp_2d)

gmm_3d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_3d = gmm_3d.fit_predict(X_wine_train_rp_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_rp_2d[:, 0], X_wine_train_rp_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with Randomized Projections (2D) - Wine Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_rp_3d[:, 0], X_wine_train_rp_3d[:, 1], X_wine_train_rp_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with Randomized Projections (3D) - Wine Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for PCA
optimal_n_components_pca = 8

# Apply PCA for 2D visualization
pca_wine_2d = PCA(n_components=2)
X_wine_train_pca_2d = pca_wine_2d.fit_transform(X_wine_train)

# Apply PCA for 3D visualization
pca_wine_3d = PCA(n_components=3)
X_wine_train_pca_3d = pca_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_2d = gmm_2d.fit_predict(X_wine_train_pca_2d)

gmm_3d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_3d = gmm_3d.fit_predict(X_wine_train_pca_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_pca_2d[:, 0], X_wine_train_pca_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with PCA (2D) - Wine Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_pca_3d[:, 0], X_wine_train_pca_3d[:, 1], X_wine_train_pca_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with PCA (3D) - Wine Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for ICA
optimal_n_components_ica = 10

# Apply ICA for 2D visualization
ica_wine_2d = FastICA(n_components=2, random_state=42)
X_wine_train_ica_2d = ica_wine_2d.fit_transform(X_wine_train)

# Apply ICA for 3D visualization
ica_wine_3d = FastICA(n_components=3, random_state=42)
X_wine_train_ica_3d = ica_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_2d = gmm_2d.fit_predict(X_wine_train_ica_2d)

gmm_3d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_3d = gmm_3d.fit_predict(X_wine_train_ica_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_ica_2d[:, 0], X_wine_train_ica_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (2D) - Wine Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_ica_3d[:, 0], X_wine_train_ica_3d[:, 1], X_wine_train_ica_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (3D) - Wine Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for ICA
optimal_n_components_ica = 10

# Apply ICA for 2D visualization
ica_wine_2d = FastICA(n_components=2, random_state=42)
X_wine_train_ica_2d = ica_wine_2d.fit_transform(X_wine_train)

# Apply ICA for 3D visualization
ica_wine_3d = FastICA(n_components=3, random_state=42)
X_wine_train_ica_3d = ica_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_2d = gmm_2d.fit_predict(X_wine_train_ica_2d)

gmm_3d = GaussianMixture(n_components=6, random_state=42)
wine_train_clusters_3d = gmm_3d.fit_predict(X_wine_train_ica_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_ica_2d[:, 0], X_wine_train_ica_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (2D) - Wine Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_ica_3d[:, 0], X_wine_train_ica_3d[:, 1], X_wine_train_ica_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (3D) - Wine Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for Randomized Projections
optimal_n_components_rp = 8

# Apply Randomized Projections for 2D visualization
rp_wine_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_wine_train_rp_2d = rp_wine_2d.fit_transform(X_wine_train)

# Apply Randomized Projections for 3D visualization
rp_wine_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_wine_train_rp_3d = rp_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_2d = kmeans_2d.fit_predict(X_wine_train_rp_2d)

kmeans_3d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_3d = kmeans_3d.fit_predict(X_wine_train_rp_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_rp_2d[:, 0], X_wine_train_rp_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with Randomized Projections (2D) - Wine Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_rp_3d[:, 0], X_wine_train_rp_3d[:, 1], X_wine_train_rp_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with Randomized Projections (3D) - Wine Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for PCA
optimal_n_components_pca = 8

# Apply PCA for 2D visualization
pca_wine_2d = PCA(n_components=2)
X_wine_train_pca_2d = pca_wine_2d.fit_transform(X_wine_train)

# Apply PCA for 3D visualization
pca_wine_3d = PCA(n_components=3)
X_wine_train_pca_3d = pca_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_2d = kmeans_2d.fit_predict(X_wine_train_pca_2d)

kmeans_3d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_3d = kmeans_3d.fit_predict(X_wine_train_pca_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_pca_2d[:, 0], X_wine_train_pca_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with PCA (2D) - Wine Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_pca_3d[:, 0], X_wine_train_pca_3d[:, 1], X_wine_train_pca_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with PCA (3D) - Wine Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the wine dataset
wine_red = pd.read_csv('/content/wine/winequality-red.csv', sep=';')
wine_white = pd.read_csv('/content/wine/winequality-white.csv', sep=';')

# Combine the two datasets
wine = pd.concat([wine_red, wine_white], ignore_index=True)

# Separate features and labels
X_wine = wine.drop(columns=['quality'])
y_wine = wine['quality']

# Normalize the features
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Split the dataset into training and testing sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X_wine_scaled, y_wine, test_size=0.2, random_state=42)

# Optimal number of components for ICA
optimal_n_components_ica = 10

# Apply ICA for 2D visualization
ica_wine_2d = FastICA(n_components=2, random_state=42)
X_wine_train_ica_2d = ica_wine_2d.fit_transform(X_wine_train)

# Apply ICA for 3D visualization
ica_wine_3d = FastICA(n_components=3, random_state=42)
X_wine_train_ica_3d = ica_wine_3d.fit_transform(X_wine_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_2d = kmeans_2d.fit_predict(X_wine_train_ica_2d)

kmeans_3d = KMeans(n_clusters=4, random_state=42)
wine_train_clusters_3d = kmeans_3d.fit_predict(X_wine_train_ica_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_wine_train, wine_train_clusters_2d)
completeness = completeness_score(y_wine_train, wine_train_clusters_2d)
v_measure = v_measure_score(y_wine_train, wine_train_clusters_2d)
ari = adjusted_rand_score(y_wine_train, wine_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Wine Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_wine_train_ica_2d[:, 0], X_wine_train_ica_2d[:, 1], c=wine_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with ICA (2D) - Wine Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Wine Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_wine_train_ica_3d[:, 0], X_wine_train_ica_3d[:, 1], X_wine_train_ica_3d[:, 2], c=wine_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with ICA (3D) - Wine Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for Randomized Projections
optimal_n_components_rp = 7

# Apply Randomized Projections for 2D visualization
rp_breast_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_breast_train_rp_2d = rp_breast_2d.fit_transform(X_breast_train)

# Apply Randomized Projections for 3D visualization
rp_breast_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_breast_train_rp_3d = rp_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_2d = gmm_2d.fit_predict(X_breast_train_rp_2d)

gmm_3d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_3d = gmm_3d.fit_predict(X_breast_train_rp_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_rp_2d[:, 0], X_breast_train_rp_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with Randomized Projections (2D) - Breast Cancer Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_rp_3d[:, 0], X_breast_train_rp_3d[:, 1], X_breast_train_rp_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with Randomized Projections (3D) - Breast Cancer Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for PCA
optimal_n_components_pca = 8

# Apply PCA for 2D visualization
pca_breast_2d = PCA(n_components=2)
X_breast_train_pca_2d = pca_breast_2d.fit_transform(X_breast_train)

# Apply PCA for 3D visualization
pca_breast_3d = PCA(n_components=3)
X_breast_train_pca_3d = pca_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_2d = gmm_2d.fit_predict(X_breast_train_pca_2d)

gmm_3d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_3d = gmm_3d.fit_predict(X_breast_train_pca_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_pca_2d[:, 0], X_breast_train_pca_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with PCA (2D) - Breast Cancer Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_pca_3d[:, 0], X_breast_train_pca_3d[:, 1], X_breast_train_pca_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with PCA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for ICA
optimal_n_components_ica = 8

# Apply ICA for 2D visualization
ica_breast_2d = FastICA(n_components=2, random_state=42)
X_breast_train_ica_2d = ica_breast_2d.fit_transform(X_breast_train)

# Apply ICA for 3D visualization
ica_breast_3d = FastICA(n_components=3, random_state=42)
X_breast_train_ica_3d = ica_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for Expectation Maximization
start_time = time.time()

# Apply Expectation Maximization
gmm_2d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_2d = gmm_2d.fit_predict(X_breast_train_ica_2d)

gmm_3d = GaussianMixture(n_components=9, random_state=42)
breast_train_clusters_3d = gmm_3d.fit_predict(X_breast_train_ica_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with EM Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_ica_2d[:, 0], X_breast_train_ica_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (2D) - Breast Cancer Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with EM Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_ica_3d[:, 0], X_breast_train_ica_3d[:, 1], X_breast_train_ica_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('EM Clustering with ICA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.random_projection import GaussianRandomProjection
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for Randomized Projections
optimal_n_components_rp = 7

# Apply Randomized Projections for 2D visualization
rp_breast_2d = GaussianRandomProjection(n_components=2, random_state=42)
X_breast_train_rp_2d = rp_breast_2d.fit_transform(X_breast_train)

# Apply Randomized Projections for 3D visualization
rp_breast_3d = GaussianRandomProjection(n_components=3, random_state=42)
X_breast_train_rp_3d = rp_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_2d = kmeans_2d.fit_predict(X_breast_train_rp_2d)

kmeans_3d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_3d = kmeans_3d.fit_predict(X_breast_train_rp_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_rp_2d[:, 0], X_breast_train_rp_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with Randomized Projections (2D) - Breast Cancer Dataset')
plt.xlabel('Random Projection Component 1')
plt.ylabel('Random Projection Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_rp_3d[:, 0], X_breast_train_rp_3d[:, 1], X_breast_train_rp_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with Randomized Projections (3D) - Breast Cancer Dataset')
ax.set_xlabel('Random Projection Component 1')
ax.set_ylabel('Random Projection Component 2')
ax.set_zlabel('Random Projection Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for PCA
optimal_n_components_pca = 8

# Apply PCA for 2D visualization
pca_breast_2d = PCA(n_components=2)
X_breast_train_pca_2d = pca_breast_2d.fit_transform(X_breast_train)

# Apply PCA for 3D visualization
pca_breast_3d = PCA(n_components=3)
X_breast_train_pca_3d = pca_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_2d = kmeans_2d.fit_predict(X_breast_train_pca_2d)

kmeans_3d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_3d = kmeans_3d.fit_predict(X_breast_train_pca_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_pca_2d[:, 0], X_breast_train_pca_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with PCA (2D) - Breast Cancer Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_pca_3d[:, 0], X_breast_train_pca_3d[:, 1], X_breast_train_pca_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with PCA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import FastICA
from sklearn.cluster import KMeans
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# Load the breast cancer dataset
breast_cancer_path = '/content/breast/breast-cancer.data'
column_names = ['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']
breast_data = pd.read_csv(breast_cancer_path, names=column_names)

# Encode categorical variables
label_encoders = {}
for column in breast_data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    breast_data[column] = label_encoders[column].fit_transform(breast_data[column])

# Separate features and labels
X_breast = breast_data.drop(columns=['Class'])
y_breast = breast_data['Class']

# Normalize the features
scaler = StandardScaler()
X_breast_scaled = scaler.fit_transform(X_breast)

# Split the dataset into training and testing sets
X_breast_train, X_breast_test, y_breast_train, y_breast_test = train_test_split(X_breast_scaled, y_breast, test_size=0.2, random_state=42)

# Optimal number of components for ICA
optimal_n_components_ica = 8

# Apply ICA for 2D visualization
ica_breast_2d = FastICA(n_components=2, random_state=42)
X_breast_train_ica_2d = ica_breast_2d.fit_transform(X_breast_train)

# Apply ICA for 3D visualization
ica_breast_3d = FastICA(n_components=3, random_state=42)
X_breast_train_ica_3d = ica_breast_3d.fit_transform(X_breast_train)

# Measure the runtime for K-Means Clustering
start_time = time.time()

# Apply K-Means Clustering
kmeans_2d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_2d = kmeans_2d.fit_predict(X_breast_train_ica_2d)

kmeans_3d = KMeans(n_clusters=2, random_state=42)
breast_train_clusters_3d = kmeans_3d.fit_predict(X_breast_train_ica_3d)

end_time = time.time()
run_time = end_time - start_time

# Calculate evaluation metrics
homogeneity = homogeneity_score(y_breast_train, breast_train_clusters_2d)
completeness = completeness_score(y_breast_train, breast_train_clusters_2d)
v_measure = v_measure_score(y_breast_train, breast_train_clusters_2d)
ari = adjusted_rand_score(y_breast_train, breast_train_clusters_2d)

print(f'Runtime: {run_time:.4f} seconds')
print(f'Homogeneity Score: {homogeneity:.4f}')
print(f'Completeness Score: {completeness:.4f}')
print(f'V-Measure: {v_measure:.4f}')
print(f'Adjusted Rand Index (ARI): {ari:.4f}')

# 2D Plot for Breast Cancer Dataset with K-Means Clustering
plt.figure(figsize=(10, 6))
plt.scatter(X_breast_train_ica_2d[:, 0], X_breast_train_ica_2d[:, 1], c=breast_train_clusters_2d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with ICA (2D) - Breast Cancer Dataset')
plt.xlabel('Independent Component 1')
plt.ylabel('Independent Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

# 3D Plot for Breast Cancer Dataset with K-Means Clustering
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_breast_train_ica_3d[:, 0], X_breast_train_ica_3d[:, 1], X_breast_train_ica_3d[:, 2], c=breast_train_clusters_3d, cmap='viridis', s=50, alpha=0.6)
plt.title('K-Means Clustering with ICA (3D) - Breast Cancer Dataset')
ax.set_xlabel('Independent Component 1')
ax.set_ylabel('Independent Component 2')
ax.set_zlabel('Independent Component 3')
plt.colorbar(sc, label='Cluster Label')
plt.show()